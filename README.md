# Assignment 8
## CS 152
## Assigned Wed., December 5, 2018
## Due Friday, December 14, 2018: 11 PM

This is an assignment for groups of one or two.  All work needs to be done together while you are 
physically present.

You will be working with both the Windy gridworld and the Cliff gridworld that we saw in class on Dec. 3.  
You should
* implement the Sarsa Temporal Difference algorithm, and
* implement the off-policy Q-Learning Temporal Difference algorithm

I've provided you starter code that implements the Windy and Cliff gridworlds.
```python assign8starter.py --help``` shows the following usage:

```
usage: assign8starter.py [-h] [--env ENV] [--alg ALG]
                         [--num_cycles NUM_CYCLES]

Solve a gridworld.

optional arguments:
  -h, --help            show this help message and exit
  --env ENV             specify the environment, one of (cliff, windy,
                        smallcliff, smallwindy)
  --alg ALG             specify the algorithm, one of (sarsa, qlearning)
  --num_cycles NUM_CYCLES
                        number of cycles to run
  --verbose             increase output verbosity
```

The ```cliff``` and ```windy``` environments implement the standard windy gridworld and cliff gridworld. 
The ```small``` versions are smaller (with fewer columns).

Running ```assign6starter.py --env smallcliff --num_cycles 1``` prints the following:
```
Q_ðœ‹_0 in order ['^', '>', 'v', '<']:
( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) 
( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) 
( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) 
( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) 
ðœ‹_1:
^ ^ ^ 
^ ^ ^ 
^ ^ ^ 
^ ^ T 

average reward/episode -9007.9
```
As you can see, it prints the Q and ðœ‹ values every so many cycles.  
It also prints the average reward/episode over the last 10 cycles.

The starter code doesn't ever update the Q function.  Your goal is to modify the code
in ```Main``` that will implement the Sarsa and Q-Learning algorithms (which one should be
controlled by the ``alg`` command-line argument).

Use a value of 1 for the discount factor (É£).  
I suggest a learning rate (âº) of 0.5, but you can 
choose it yourself.

You will be doing four combinations:
* ```--env windy --alg sarsa```
* ```--env windy --alg qlearning```
* ```--env cliff --alg sarsa```
* ```--env cliff --alg qlearning```

You'll probably need  different values of ```num_cycles``` values for each combination.

You should verify that both solutions to the Windy gridworld find the correct optimal policy
(the same path as was shown in the slides in class).

In addition, you should verify that the policy generated by the Sarsa algorithm is more prudent, and has
a higher average reward/cycle than does the policy generated by the Q-Learning algorithm. 
The policies for the "happy path" (following the policy arrows all the way to the target) should match those
shown in class.

Turn in five files:
* ```assignment8.py```
* ```assignment8_sarsa_windy.out```
* ```assignment8_qlearning_windy.out```
* ```assignment8_sarsa_cliff.out```
* ```assignment8_qlearning_cliff.out```

All five files should have your name(s) within the first few lines.
The ```.out``` files should contain a comment at the top line showing the command-line option used to
create the output.
The remainder of the file should be the last q values, pi values, and average reward generated 
(showing the correct policies). There's no need to show any intermediate values.

Here's an example:
```
# python starter8_solution.py --env windy --alg sarsa --num_cycles 1000
# by Neil Rhodes

Q_ðœ‹_990 in order ['^', '>', 'v', '<']:
(-18.86 -18.04 -18.89 -18.46) (-17.65 -16.01 -18.16 -18.15) (-16.43 -15.02 -16.36 -17.22) (-16.50 -15.01 -16.05 -15.92) (-15.28 -13.64 -15.21 -15.37) (-13.29 -11.18 -13.07 -14.92) (-12.05 -10.14 -13.24 -12.90) (-11.47 -9.17 -11.44 -13.21) (-10.94 -8.10 -10.32 -11.82) (-11.52 -10.51 -6.79 -12.26) 
(-19.04 -18.89 -19.10 -19.00) (-17.70 -18.09 -18.19 -18.83) (-17.28 -15.95 -17.34 -18.12) (-16.11 -14.24 -15.49 -16.25) (-14.54 -13.68 -14.16 -15.44) (-13.71 -11.18 -13.32 -13.74) (-13.24 -11.56 -12.53 -14.13) (-11.67 -11.17 -11.39 -11.85) (-9.91 -10.93 -11.21 -10.77) (-11.21 -7.39 -5.35 -11.42) 
(-19.59 -18.27 -20.26 -20.11) (-18.57 -16.61 -18.52 -19.64) (-17.72 -15.20 -16.88 -18.76) (-15.38 -14.00 -15.51 -17.07) (-15.55 -12.47 -14.92 -14.53) (-14.57 -12.64 -14.73 -14.33) (-12.04 -10.06 -12.25 -12.84) (-10.69 -9.83 -10.85 -10.20) (-10.73 -7.64 -8.99 -10.38) (-9.16 -8.47 -4.07 -8.79) 
(-19.65 -18.01 -19.58 -19.69) (-18.61 -16.88 -19.18 -20.76) (-17.42 -15.60 -16.84 -18.60) (-15.44 -14.04 -15.58 -16.25) (-15.44 -14.31 -15.34 -15.29) (-13.54 -11.14 -13.32 -13.77) (-12.37 -12.00 -12.46 -12.60) ( 0.00  0.00  0.00  0.00) (-10.61 -8.57 -10.59 -10.39) (-7.44 -5.86 -3.00 -8.94) 
(-19.56 -17.94 -19.67 -20.15) (-19.12 -17.38 -17.66 -19.36) (-17.52 -16.08 -16.40 -18.49) (-15.11 -15.76 -16.42 -16.13) (-14.06 -12.13 -14.13 -14.70) (-12.95 -12.81 -13.61 -14.54) ( 0.00  0.00  0.00  0.00) (-10.90 -7.79 -1.00 -7.00) (-8.94 -4.60 -2.00 -1.00) (-6.16 -5.07 -7.57 -2.00) 
(-19.84 -17.75 -18.35 -18.75) (-17.94 -16.38 -17.62 -17.65) (-17.09 -14.78 -16.54 -17.41) (-15.56 -13.36 -15.16 -16.90) (-15.03 -13.88 -14.15 -14.67) ( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) (-6.55 -3.98 -2.00 -10.64) (-9.17 -3.45 -8.11 -6.99) (-7.57 -7.91 -7.28 -5.70) 
(-18.25 -17.93 -17.98 -18.53) (-17.71 -17.18 -17.83 -17.98) (-16.17 -15.64 -17.22 -17.58) (-15.20 -14.85 -15.20 -15.68) ( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) ( 0.00  0.00  0.00  0.00) (-2.52 -6.67 -6.90 -6.40) (-7.53 -7.59 -7.36 -5.03) 
ðœ‹_991:
> > > > > > > > > v 
> ^ > > > > > > ^ v 
> > > > > > > > > v 
> > > > > > > T > v 
> > > ^ > > ^ v < < 
> > > > > ^ ^ v > < 
> > > > ^ ^ ^ ^ ^ < 
```
